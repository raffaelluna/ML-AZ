{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '~/ML-AZ/census.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_census = df.iloc[:, 0:14].values\n",
    "y_census = df.iloc[:, 14].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 108)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#OneHotEncoder cria uma coluna dummy para cada atributo nominal único, de\n",
    "#modo a não criar uma ordem ao atribuir labels do tipo 0, 1, 3, 4...\n",
    "#ColumnTransformer transforma seletivamente as colunas de um array multi-atributos\n",
    "onehotencorder = ColumnTransformer(transformers=[(\"OneHot\", OneHotEncoder(), [1,3,5,6,7,8,9,13])],remainder='passthrough')\n",
    "x_census = onehotencorder.fit_transform(x_census).toarray()\n",
    "print(x_census.shape)\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "y_census = labelencoder.fit_transform(y_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_census = scaler.fit_transform(x_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_census, x_test_census, y_train_census, y_test_census = train_test_split(\n",
    "    x_census, \n",
    "    y_census, \n",
    "    test_size = 0.15, \n",
    "    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37821541\n",
      "Iteration 2, loss = 0.32553691\n",
      "Iteration 3, loss = 0.31552736\n",
      "Iteration 4, loss = 0.30833053\n",
      "Iteration 5, loss = 0.30384866\n",
      "Iteration 6, loss = 0.29959929\n",
      "Iteration 7, loss = 0.29685337\n",
      "Iteration 8, loss = 0.29458778\n",
      "Iteration 9, loss = 0.29208632\n",
      "Iteration 10, loss = 0.28962088\n",
      "Iteration 11, loss = 0.28751667\n",
      "Iteration 12, loss = 0.28633369\n",
      "Iteration 13, loss = 0.28440160\n",
      "Iteration 14, loss = 0.28367864\n",
      "Iteration 15, loss = 0.28144986\n",
      "Iteration 16, loss = 0.28056185\n",
      "Iteration 17, loss = 0.27912207\n",
      "Iteration 18, loss = 0.27804390\n",
      "Iteration 19, loss = 0.27651886\n",
      "Iteration 20, loss = 0.27572158\n",
      "Iteration 21, loss = 0.27470324\n",
      "Iteration 22, loss = 0.27374115\n",
      "Iteration 23, loss = 0.27261076\n",
      "Iteration 24, loss = 0.27143228\n",
      "Iteration 25, loss = 0.26952248\n",
      "Iteration 26, loss = 0.26903171\n",
      "Iteration 27, loss = 0.26901350\n",
      "Iteration 28, loss = 0.26799121\n",
      "Iteration 29, loss = 0.26695708\n",
      "Iteration 30, loss = 0.26605789\n",
      "Iteration 31, loss = 0.26464549\n",
      "Iteration 32, loss = 0.26449388\n",
      "Iteration 33, loss = 0.26379163\n",
      "Iteration 34, loss = 0.26338559\n",
      "Iteration 35, loss = 0.26237648\n",
      "Iteration 36, loss = 0.26112591\n",
      "Iteration 37, loss = 0.26012097\n",
      "Iteration 38, loss = 0.26016652\n",
      "Iteration 39, loss = 0.26047180\n",
      "Iteration 40, loss = 0.25934140\n",
      "Iteration 41, loss = 0.25761790\n",
      "Iteration 42, loss = 0.25754916\n",
      "Iteration 43, loss = 0.25683885\n",
      "Iteration 44, loss = 0.25619258\n",
      "Iteration 45, loss = 0.25550747\n",
      "Iteration 46, loss = 0.25541181\n",
      "Iteration 47, loss = 0.25446977\n",
      "Iteration 48, loss = 0.25394242\n",
      "Iteration 49, loss = 0.25258520\n",
      "Iteration 50, loss = 0.25260385\n",
      "Iteration 51, loss = 0.25248701\n",
      "Iteration 52, loss = 0.25158868\n",
      "Iteration 53, loss = 0.25061694\n",
      "Iteration 54, loss = 0.24955274\n",
      "Iteration 55, loss = 0.25067162\n",
      "Iteration 56, loss = 0.24966133\n",
      "Iteration 57, loss = 0.24835137\n",
      "Iteration 58, loss = 0.24813220\n",
      "Iteration 59, loss = 0.24741587\n",
      "Iteration 60, loss = 0.24713759\n",
      "Iteration 61, loss = 0.24679781\n",
      "Iteration 62, loss = 0.24700481\n",
      "Iteration 63, loss = 0.24521175\n",
      "Iteration 64, loss = 0.24644524\n",
      "Iteration 65, loss = 0.24552726\n",
      "Iteration 66, loss = 0.24479540\n",
      "Iteration 67, loss = 0.24344246\n",
      "Iteration 68, loss = 0.24395906\n",
      "Iteration 69, loss = 0.24345088\n",
      "Iteration 70, loss = 0.24324040\n",
      "Iteration 71, loss = 0.24248950\n",
      "Iteration 72, loss = 0.24166726\n",
      "Iteration 73, loss = 0.24181792\n",
      "Iteration 74, loss = 0.24058685\n",
      "Iteration 75, loss = 0.24053908\n",
      "Iteration 76, loss = 0.24028113\n",
      "Iteration 77, loss = 0.24039229\n",
      "Iteration 78, loss = 0.23997464\n",
      "Iteration 79, loss = 0.23904663\n",
      "Iteration 80, loss = 0.23810195\n",
      "Iteration 81, loss = 0.23929154\n",
      "Iteration 82, loss = 0.23785317\n",
      "Iteration 83, loss = 0.23820847\n",
      "Iteration 84, loss = 0.23696350\n",
      "Iteration 85, loss = 0.23794116\n",
      "Iteration 86, loss = 0.23579781\n",
      "Iteration 87, loss = 0.23564906\n",
      "Iteration 88, loss = 0.23592851\n",
      "Iteration 89, loss = 0.23567269\n",
      "Iteration 90, loss = 0.23579253\n",
      "Iteration 91, loss = 0.23433594\n",
      "Iteration 92, loss = 0.23413974\n",
      "Iteration 93, loss = 0.23404211\n",
      "Iteration 94, loss = 0.23318004\n",
      "Iteration 95, loss = 0.23363442\n",
      "Iteration 96, loss = 0.23346496\n",
      "Iteration 97, loss = 0.23308851\n",
      "Iteration 98, loss = 0.23204008\n",
      "Iteration 99, loss = 0.23226543\n",
      "Iteration 100, loss = 0.23103572\n",
      "Iteration 101, loss = 0.23144237\n",
      "Iteration 102, loss = 0.23117394\n",
      "Iteration 103, loss = 0.23067574\n",
      "Iteration 104, loss = 0.23044454\n",
      "Iteration 105, loss = 0.23106610\n",
      "Iteration 106, loss = 0.23060061\n",
      "Iteration 107, loss = 0.22907153\n",
      "Iteration 108, loss = 0.22962837\n",
      "Iteration 109, loss = 0.22918291\n",
      "Iteration 110, loss = 0.22831872\n",
      "Iteration 111, loss = 0.22860558\n",
      "Iteration 112, loss = 0.22852608\n",
      "Iteration 113, loss = 0.22780929\n",
      "Iteration 114, loss = 0.22733111\n",
      "Iteration 115, loss = 0.22802049\n",
      "Iteration 116, loss = 0.22684921\n",
      "Iteration 117, loss = 0.22631087\n",
      "Iteration 118, loss = 0.22733907\n",
      "Iteration 119, loss = 0.22648388\n",
      "Iteration 120, loss = 0.22606087\n",
      "Iteration 121, loss = 0.22602681\n",
      "Iteration 122, loss = 0.22517127\n",
      "Iteration 123, loss = 0.22510651\n",
      "Iteration 124, loss = 0.22536285\n",
      "Iteration 125, loss = 0.22705815\n",
      "Iteration 126, loss = 0.22528608\n",
      "Iteration 127, loss = 0.22426004\n",
      "Iteration 128, loss = 0.22516250\n",
      "Iteration 129, loss = 0.22425135\n",
      "Iteration 130, loss = 0.22454581\n",
      "Iteration 131, loss = 0.22338763\n",
      "Iteration 132, loss = 0.22279767\n",
      "Iteration 133, loss = 0.22266878\n",
      "Iteration 134, loss = 0.22224887\n",
      "Iteration 135, loss = 0.22276247\n",
      "Iteration 136, loss = 0.22151638\n",
      "Iteration 137, loss = 0.22239812\n",
      "Iteration 138, loss = 0.22225360\n",
      "Iteration 139, loss = 0.22070772\n",
      "Iteration 140, loss = 0.22184856\n",
      "Iteration 141, loss = 0.21990184\n",
      "Iteration 142, loss = 0.22106833\n",
      "Iteration 143, loss = 0.21952996\n",
      "Iteration 144, loss = 0.22067078\n",
      "Iteration 145, loss = 0.21967062\n",
      "Iteration 146, loss = 0.21959875\n",
      "Iteration 147, loss = 0.21897350\n",
      "Iteration 148, loss = 0.21991801\n",
      "Iteration 149, loss = 0.21954661\n",
      "Iteration 150, loss = 0.21862256\n",
      "Iteration 151, loss = 0.21937513\n",
      "Iteration 152, loss = 0.21882019\n",
      "Iteration 153, loss = 0.21813394\n",
      "Iteration 154, loss = 0.21887125\n",
      "Iteration 155, loss = 0.21931406\n",
      "Iteration 156, loss = 0.21813345\n",
      "Iteration 157, loss = 0.21703387\n",
      "Iteration 158, loss = 0.21738601\n",
      "Iteration 159, loss = 0.21793846\n",
      "Iteration 160, loss = 0.21620873\n",
      "Iteration 161, loss = 0.21669407\n",
      "Iteration 162, loss = 0.21632266\n",
      "Iteration 163, loss = 0.21710578\n",
      "Iteration 164, loss = 0.21497065\n",
      "Iteration 165, loss = 0.21546213\n",
      "Iteration 166, loss = 0.21579819\n",
      "Iteration 167, loss = 0.21521702\n",
      "Iteration 168, loss = 0.21643324\n",
      "Iteration 169, loss = 0.21570735\n",
      "Iteration 170, loss = 0.21482876\n",
      "Iteration 171, loss = 0.21525909\n",
      "Iteration 172, loss = 0.21535560\n",
      "Iteration 173, loss = 0.21519892\n",
      "Iteration 174, loss = 0.21520818\n",
      "Iteration 175, loss = 0.21389607\n",
      "Iteration 176, loss = 0.21379833\n",
      "Iteration 177, loss = 0.21354507\n",
      "Iteration 178, loss = 0.21470034\n",
      "Iteration 179, loss = 0.21371012\n",
      "Iteration 180, loss = 0.21361513\n",
      "Iteration 181, loss = 0.21346777\n",
      "Iteration 182, loss = 0.21321226\n",
      "Iteration 183, loss = 0.21307902\n",
      "Iteration 184, loss = 0.21201618\n",
      "Iteration 185, loss = 0.21382532\n",
      "Iteration 186, loss = 0.21279137\n",
      "Iteration 187, loss = 0.21255309\n",
      "Iteration 188, loss = 0.21045862\n",
      "Iteration 189, loss = 0.21181120\n",
      "Iteration 190, loss = 0.21125102\n",
      "Iteration 191, loss = 0.21126542\n",
      "Iteration 192, loss = 0.21117390\n",
      "Iteration 193, loss = 0.21375604\n",
      "Iteration 194, loss = 0.21044415\n",
      "Iteration 195, loss = 0.21023595\n",
      "Iteration 196, loss = 0.21009417\n",
      "Iteration 197, loss = 0.20910250\n",
      "Iteration 198, loss = 0.20976617\n",
      "Iteration 199, loss = 0.20947058\n",
      "Iteration 200, loss = 0.20909297\n",
      "Iteration 201, loss = 0.20905576\n",
      "Iteration 202, loss = 0.20941017\n",
      "Iteration 203, loss = 0.20936371\n",
      "Iteration 204, loss = 0.20956286\n",
      "Iteration 205, loss = 0.20901181\n",
      "Iteration 206, loss = 0.20819576\n",
      "Iteration 207, loss = 0.21135737\n",
      "Iteration 208, loss = 0.20937195\n",
      "Iteration 209, loss = 0.20751746\n",
      "Iteration 210, loss = 0.20807279\n",
      "Iteration 211, loss = 0.20674446\n",
      "Iteration 212, loss = 0.20866567\n",
      "Iteration 213, loss = 0.20849773\n",
      "Iteration 214, loss = 0.20663127\n",
      "Iteration 215, loss = 0.20741010\n",
      "Iteration 216, loss = 0.20673412\n",
      "Iteration 217, loss = 0.20595033\n",
      "Iteration 218, loss = 0.20642407\n",
      "Iteration 219, loss = 0.20518296\n",
      "Iteration 220, loss = 0.20772868\n",
      "Iteration 221, loss = 0.20691695\n",
      "Iteration 222, loss = 0.20550703\n",
      "Iteration 223, loss = 0.20577114\n",
      "Iteration 224, loss = 0.20548790\n",
      "Iteration 225, loss = 0.20592092\n",
      "Iteration 226, loss = 0.20543756\n",
      "Iteration 227, loss = 0.20519404\n",
      "Iteration 228, loss = 0.20531334\n",
      "Iteration 229, loss = 0.20442726\n",
      "Iteration 230, loss = 0.20547109\n",
      "Iteration 231, loss = 0.20459704\n",
      "Iteration 232, loss = 0.20425073\n",
      "Iteration 233, loss = 0.20396036\n",
      "Iteration 234, loss = 0.20377417\n",
      "Iteration 235, loss = 0.20399450\n",
      "Iteration 236, loss = 0.20384222\n",
      "Iteration 237, loss = 0.20466120\n",
      "Iteration 238, loss = 0.20346276\n",
      "Iteration 239, loss = 0.20286543\n",
      "Iteration 240, loss = 0.20296925\n",
      "Iteration 241, loss = 0.20254670\n",
      "Iteration 242, loss = 0.20271218\n",
      "Iteration 243, loss = 0.20274860\n",
      "Iteration 244, loss = 0.20241933\n",
      "Iteration 245, loss = 0.20243717\n",
      "Iteration 246, loss = 0.20185329\n",
      "Iteration 247, loss = 0.20253097\n",
      "Iteration 248, loss = 0.20200922\n",
      "Iteration 249, loss = 0.20128204\n",
      "Iteration 250, loss = 0.20072989\n",
      "Iteration 251, loss = 0.20232717\n",
      "Iteration 252, loss = 0.20096802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.20144486\n",
      "Iteration 254, loss = 0.20104148\n",
      "Iteration 255, loss = 0.20135090\n",
      "Iteration 256, loss = 0.19982126\n",
      "Iteration 257, loss = 0.20094018\n",
      "Iteration 258, loss = 0.20185740\n",
      "Iteration 259, loss = 0.20160291\n",
      "Iteration 260, loss = 0.20065497\n",
      "Iteration 261, loss = 0.20128527\n",
      "Iteration 262, loss = 0.19967116\n",
      "Iteration 263, loss = 0.19973101\n",
      "Iteration 264, loss = 0.19931105\n",
      "Iteration 265, loss = 0.19923286\n",
      "Iteration 266, loss = 0.19847004\n",
      "Iteration 267, loss = 0.19928321\n",
      "Iteration 268, loss = 0.19842343\n",
      "Iteration 269, loss = 0.19908798\n",
      "Iteration 270, loss = 0.19963253\n",
      "Iteration 271, loss = 0.19909288\n",
      "Iteration 272, loss = 0.19839886\n",
      "Iteration 273, loss = 0.19874199\n",
      "Iteration 274, loss = 0.19881680\n",
      "Iteration 275, loss = 0.19937927\n",
      "Iteration 276, loss = 0.19774794\n",
      "Iteration 277, loss = 0.19858596\n",
      "Iteration 278, loss = 0.19799898\n",
      "Iteration 279, loss = 0.19725699\n",
      "Iteration 280, loss = 0.19851036\n",
      "Iteration 281, loss = 0.19745130\n",
      "Iteration 282, loss = 0.19809695\n",
      "Iteration 283, loss = 0.19643221\n",
      "Iteration 284, loss = 0.19661909\n",
      "Iteration 285, loss = 0.19626357\n",
      "Iteration 286, loss = 0.19635663\n",
      "Iteration 287, loss = 0.19652691\n",
      "Iteration 288, loss = 0.19708854\n",
      "Iteration 289, loss = 0.19638330\n",
      "Iteration 290, loss = 0.19601990\n",
      "Iteration 291, loss = 0.19730291\n",
      "Iteration 292, loss = 0.19684839\n",
      "Iteration 293, loss = 0.19736898\n",
      "Iteration 294, loss = 0.19671693\n",
      "Iteration 295, loss = 0.19602002\n",
      "Iteration 296, loss = 0.19720666\n",
      "Iteration 297, loss = 0.19594634\n",
      "Iteration 298, loss = 0.19562285\n",
      "Iteration 299, loss = 0.19512503\n",
      "Iteration 300, loss = 0.19645773\n",
      "Iteration 301, loss = 0.19458141\n",
      "Iteration 302, loss = 0.19515885\n",
      "Iteration 303, loss = 0.19518077\n",
      "Iteration 304, loss = 0.19490248\n",
      "Iteration 305, loss = 0.19451570\n",
      "Iteration 306, loss = 0.19453771\n",
      "Iteration 307, loss = 0.19454150\n",
      "Iteration 308, loss = 0.19456325\n",
      "Iteration 309, loss = 0.19596141\n",
      "Iteration 310, loss = 0.19485206\n",
      "Iteration 311, loss = 0.19439056\n",
      "Iteration 312, loss = 0.19355357\n",
      "Iteration 313, loss = 0.19517491\n",
      "Iteration 314, loss = 0.19392085\n",
      "Iteration 315, loss = 0.19474783\n",
      "Iteration 316, loss = 0.19297948\n",
      "Iteration 317, loss = 0.19374296\n",
      "Iteration 318, loss = 0.19339546\n",
      "Iteration 319, loss = 0.19319444\n",
      "Iteration 320, loss = 0.19278455\n",
      "Iteration 321, loss = 0.19253757\n",
      "Iteration 322, loss = 0.19252520\n",
      "Iteration 323, loss = 0.19308674\n",
      "Iteration 324, loss = 0.19286937\n",
      "Iteration 325, loss = 0.19226084\n",
      "Iteration 326, loss = 0.19307986\n",
      "Iteration 327, loss = 0.19233823\n",
      "Iteration 328, loss = 0.19209109\n",
      "Iteration 329, loss = 0.19224418\n",
      "Iteration 330, loss = 0.19150299\n",
      "Iteration 331, loss = 0.19263314\n",
      "Iteration 332, loss = 0.19306052\n",
      "Iteration 333, loss = 0.19322935\n",
      "Iteration 334, loss = 0.19079630\n",
      "Iteration 335, loss = 0.19121113\n",
      "Iteration 336, loss = 0.19172023\n",
      "Iteration 337, loss = 0.19069207\n",
      "Iteration 338, loss = 0.19129894\n",
      "Iteration 339, loss = 0.19109557\n",
      "Iteration 340, loss = 0.19164773\n",
      "Iteration 341, loss = 0.19027464\n",
      "Iteration 342, loss = 0.19110432\n",
      "Iteration 343, loss = 0.19059434\n",
      "Iteration 344, loss = 0.19182027\n",
      "Iteration 345, loss = 0.19104443\n",
      "Iteration 346, loss = 0.19111894\n",
      "Iteration 347, loss = 0.19058205\n",
      "Iteration 348, loss = 0.18966644\n",
      "Iteration 349, loss = 0.18932797\n",
      "Iteration 350, loss = 0.19019981\n",
      "Iteration 351, loss = 0.18958763\n",
      "Iteration 352, loss = 0.19042518\n",
      "Iteration 353, loss = 0.19013105\n",
      "Iteration 354, loss = 0.19038644\n",
      "Iteration 355, loss = 0.18944450\n",
      "Iteration 356, loss = 0.18861632\n",
      "Iteration 357, loss = 0.18822213\n",
      "Iteration 358, loss = 0.18881363\n",
      "Iteration 359, loss = 0.18822813\n",
      "Iteration 360, loss = 0.18920263\n",
      "Iteration 361, loss = 0.18850671\n",
      "Iteration 362, loss = 0.19001433\n",
      "Iteration 363, loss = 0.18945795\n",
      "Iteration 364, loss = 0.18903327\n",
      "Iteration 365, loss = 0.19039343\n",
      "Iteration 366, loss = 0.18897653\n",
      "Iteration 367, loss = 0.18755017\n",
      "Iteration 368, loss = 0.18756583\n",
      "Iteration 369, loss = 0.18630674\n",
      "Iteration 370, loss = 0.18779312\n",
      "Iteration 371, loss = 0.18716297\n",
      "Iteration 372, loss = 0.18764558\n",
      "Iteration 373, loss = 0.18783388\n",
      "Iteration 374, loss = 0.18658724\n",
      "Iteration 375, loss = 0.18666732\n",
      "Iteration 376, loss = 0.18779217\n",
      "Iteration 377, loss = 0.18720942\n",
      "Iteration 378, loss = 0.18710127\n",
      "Iteration 379, loss = 0.18642267\n",
      "Iteration 380, loss = 0.18743134\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(verbose = True, max_iter=1000, tol=0.000010)\n",
    "clf.fit(x_train_census, y_train_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8227226202661208"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = clf.predict(x_test_census)\n",
    "print(clf.out_activation_)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "precisao = accuracy_score(y_test_census, resultado)\n",
    "matriz = confusion_matrix(y_test_census, resultado)\n",
    "precisao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
